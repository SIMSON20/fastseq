# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/03_data.load.ipynb (unless otherwise specified).

__all__ = ['TSDataLoader', 'TSBlock', 'concat_ts_list', 'make_test', 'make_test_pct', 'TSDataBunch']

# Cell
from ..core import *
from .external import *
from fastcore.utils import *
from fastcore.imports import *
from fastai2.basics import *
from fastai2.data.transforms import *
from fastai2.tabular.core import *
from .transforms import *

# Cell
import numpy as np
import pandas as pd
from torch.utils.data import Dataset, DataLoader

# Cell
# TODO maybe incl. start where the last one ended and therefor keep hidden state
@delegates()
class TSDataLoader(TfmdDL):
    def __init__(self, items, horizon, lookback=72, step=1, bs=64, norm=True,
                 num_workers=0, after_batch=None, device=None, after_item = None, **kwargs):
        self.horizon, self.lookback, self.step = horizon, lookback, step
        self.items = self.norm_items(items, norm)
        n = self.make_ids()
        after_batch = ifnone(after_batch, noop)
        after_item = ifnone(after_item, noop)
        super().__init__(dataset=items, bs=bs, num_workers=num_workers, after_batch=after_batch,
                         after_item=after_item, **kwargs)
        self.n = n

    def norm_items(self, items, norm):
        items = items.map(tensor)
        r=L()
        for i,ts in enumerate(items):
            ts = ts.float()
            if norm:
                ts = (ts - torch.mean(ts.float(), -1, keepdim = True))/(torch.std(ts.float(), -1, keepdim = True)+1e-8)
            r.append(ts)
        return r

    def make_ids(self):
        # Slice each time series into examples, assigning IDs to each
        last_id = 0
        n_dropped = 0
        n_needs_padding = 0
        self._ids = {}
        for i, ts in enumerate(self.items):
            if isinstance(ts,tuple):
                ts = ts[0] # no idea why they become tuples
            num_examples = (ts.shape[-1] - self.lookback - self.horizon + self.step) // self.step
            # Time series shorter than the forecast horizon need to be dropped.
            if ts.shape[-1] < self.horizon:
                n_dropped += 1
                continue
            # For short time series zero pad the input
            if ts.shape[-1] < self.lookback + self.horizon:
                n_needs_padding += 1
                num_examples = 1
            for j in range(num_examples):
                self._ids[last_id + j] = (i, j * self.step)
            last_id += num_examples

        # Inform user about time series that were too short
        if n_dropped > 0:
            print("Dropped {}/{} time series due to length.".format(
                    n_dropped, len(self.items)))

        # Inform user about time series that were short
        if n_needs_padding > 0:
            print("Need to pad {}/{} time series due to length.".format(
                    n_needs_padding, len(self.items)))
        # Store the number of training examples
        return int(self._ids.__len__() )

    def get_id(self,idx):
        # Get time series
        ts_id, lookback_id = self._ids[idx]
        ts = self.items[ts_id]
        if isinstance(ts,tuple):
            ts = ts[0] # no idea why they become tuples
        # Prepare input and target. Zero pad if necessary.
        if ts.shape[-1] < self.lookback + self.horizon:
            # If the time series is too short, we zero pad
            x = ts[:, :-self.horizon]
            x = np.pad(
                x,
                pad_width=((0, 0), (self.lookback - x.shape[-1], 0)),
                mode='constant',
                constant_values=0
            )
            y = ts[:,-self.horizon:]
        else:
            x = ts[:,lookback_id:lookback_id + self.lookback]
            y = ts[:,lookback_id + self.lookback:lookback_id + self.lookback + self.horizon]
        return x, y

    def shuffle_fn(self, idxs):
        self.items.shuffle()
        return idxs

    def create_item(self, idx):
        if idx>=self.n: raise IndexError
        x, y = self.get_id(idx)
        return TSTensorSeq(x),TSTensorSeqy(y, x_len=x.shape[1], m='-*g')


# Cell

from fastai2.vision.data import *

@typedispatch
def show_batch(x: TensorSeq, y, samples, ctxs=None, max_n=10,rows=None, cols=None, figsize=None, **kwargs):
    if ctxs is None: ctxs = get_grid(min(len(samples), max_n), rows=rows, cols=cols, add_vert=1, figsize=figsize)
    ctxs = show_batch[object](x, y, samples=samples, ctxs=ctxs, max_n=max_n, **kwargs)
    return ctxs


# Cell
def TSBlock():
    return TransformBlock(dl_type=TSDataLoader,)

# Cell
def concat_ts_list(train, val):
    items=L()
    assert len(train) == len(val)
    for t, v in zip(train, val):
        items.append(np.concatenate([t,v],1))
    return items

# Cell
def make_test(items:L(), horizon:int, lookback:int, keep_lookback:bool = False):
    """Splits the every ts in `items` based on `horizon + lookback`*, where the last part will go into `val` and the first in `train`.

    *if `keep_lookback`:
        it will only remove `horizon` from `train` otherwise also lookback.
    """
    train, val = L(), L()
    for ts in items:
        val.append(ts[:, -(horizon+lookback):])
        if keep_lookback:
            train.append(ts[:, :-(horizon)])
        else:
            train.append(ts[:, :-(horizon+lookback)])

    return train, val

def make_test_pct(items:L(), pct:float):
    """Splits the every ts in `items` based on `pct`(percentage) of the length of the timeserie, where the last part will go into `val` and the first in `train`.

    """
    train, val = L(), L()
    for ts in items:
        split_idx = int((1-pct)*ts.shape[1])
        train.append(ts[:,:split_idx])
        val.append(ts[:,split_idx:])

    return train, val

# Cell
class TSDataBunch(DataBunch):
    @classmethod
    @delegates(DataBunch.__init__)
    def from_folder(cls, path, valid_pct=.5, seed=None, horizon=None, lookback=None, step=1, device=None,
                   nrows=None, skiprows=None, **kwargs):
        """Create from M-compition style in `path` with `train`,`test` csv-files.

        The `DataLoader` for the test set will be save as an attribute under `test_dl`
        """
        train, test = get_ts_files(path, nrows=nrows, skiprows=skiprows)
        items = concat_ts_list(train, test)
        horizon = ifnone(horizon, len(test[0]))
        lookback = ifnone(lookback, horizon * 3)
        return cls.from_items(items, horizon, lookback = lookback, path=path, step = step, device = device)


    @classmethod
    @delegates(DataBunch.__init__)
    def from_items(cls, items:L, horizon:int, path:Path='.', valid_pct=.5, seed=None, lookback=None, step=1,
                   device=None, **kwargs):
        """Create an list of time series.

        The `DataLoader` for the test set will be save as an attribute under `test_dl`
        """
        lookback = ifnone(lookback, horizon * 4)
        items, test = make_test(items, horizon, lookback, keep_lookback = True)
        train, valid = make_test(items, int(lookback*valid_pct), lookback, keep_lookback = True)
        items = L(*train,*valid,*test)
        splits = IndexsSplitter(len(train),len(train)+len(valid), True)(items)
        dsrc = DataSource(items, noop, splits=splits, dl_type=TSDataLoader)
        db = dsrc.databunch(horizon=horizon, lookback=lookback, step=step, device=device, **kwargs)
        db.test_dl = TSDataLoader(test, horizon=horizon, lookback=lookback, step=step, device=device)
        print(f"Train:{db.train_dl.n}; Valid: {db.valid_dl.n}; Test {db.test_dl.n}")
#         TODO add with test_dl, currently give buges
        return db