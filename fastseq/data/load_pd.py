# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/020_data.load_pd.ipynb (unless otherwise specified).

__all__ = ['DfDataLoader']

# Cell
from ..core import *
from .external import *
from fastcore.utils import *
from fastcore.imports import *
from fastai2.basics import *
from fastai2.data.transforms import *
from fastai2.tabular.core import *

# Cell
import numpy as np
import pandas as pd

# Cell
from typing import List
def same_size_ts(ts:pd.Series, ts_names:List[str]):
    all_same = [[(ts[c].shape == ts[a].shape) for c in ts_names] for a in ts_names]
    mask = np.array(all_same)
    return np.sum(mask) == len(ts_names)**2

# Cell
@delegates()
class DfDataLoader(TfmdDL):
    def __init__(self, dataset:pd.DataFrame, y_name, horizon, lookback=72, step=1, min_seq_len=None, max_std= 2, norm=True, **kwargs):
        store_attr(self,'horizon,lookback,step,max_std,norm,y_name')
        self.min_seq_len = ifnone(min_seq_len, lookback)
        self.dataset = dataset
        self.con_names, self.cat_names, self.ts_names = L(), L(), L()
        for col in dataset.columns:
            t = type(dataset[col].iloc[0])
            print(t)
            if t is pd.core.series.Series:
                self.ts_names.append(col)
            elif isinstance(dataset[col].iloc[0], int) or t is np.int64:
                self.con_names.append(col)
            elif isinstance(dataset[col].iloc[0], float):
                self.con_names.append(col)
            else:
                raise Exception(t)
        assert y_name in self.ts_names
        n = self.make_ids()
        super().__init__(dataset=self.dataset, **kwargs)
        self.n = n
        self.skipped= []
        self.ms = {}

    @delegates(TfmdDL.new)
    def new(self, dataset=None, cls=None, **kwargs):
        res = super().new(dataset, cls, horizon=self.horizon, lookback=self.lookback, step=self.step , **kwargs)
        res.make_ids()
        return res

    def make_ids(self):
        """Make ids if the sequence is shorter than `min_seq_len`, it will drop that sequence."""
        # Slice each time series into examples, assigning IDs to each
        last_id = 0
        n_dropped = 0
        n_needs_padding = 0
        self._ids = {}
        for i, ts in self.dataset.iterrows():
            assert same_size_ts(ts, self.ts_names), f"row {i} are not all the time series the same length"
            num_examples = (ts[self.y_name].shape[-1] - self.lookback - self.horizon + self.step) // self.step
            # Time series shorter than the forecast horizon need to be dropped.
            if ts[self.y_name].shape[-1] < self.min_seq_len:
                n_dropped += 1
                continue
            # For short time series zero pad the input
            if ts[self.y_name].shape[-1] < self.lookback + self.horizon:
                n_needs_padding += 1
                num_examples = 1
            for j in range(num_examples):
                self._ids[last_id + j] = (i, j * self.step)
            last_id += num_examples

        # Inform user about time series that were too short
        if n_dropped > 0:
            print("Dropped {}/{} time series due to length.".format(
                    n_dropped, len(self.dataset)))

        # Inform user about time series that were short
        if n_needs_padding > 0:
            print("Need to pad {}/{} time series due to length.".format(
                    n_needs_padding, len(self.dataset)))
        # Store the number of training examples
        return int(self._ids.__len__() )
#         def shuffle_fn(self, idxs):
# #         self.dataset.shuffle()
#         return idxs

#     def get_id(self, idx):
#         # Get time series
#         ts_id, lookback_id = self._ids[idx]
#         ts = self.dataset[ts_id]
#         if isinstance(ts,tuple):
#             ts = ts[0] # no idea why they become tuples
#         # Prepare input and target. Zero pad if necessary.
#         if ts.shape[-1] < self.lookback + self.horizon:
#             # If the time series is too short, we zero pad
#             x = ts[:, :-self.horizon]
#             mean = x.mean()
#             x = tensor(np.pad(
#                 x,
#                 pad_width=((0, 0), (self.lookback - x.shape[-1], 0)),
#                 mode='constant',
#                 constant_values=mean
#             ))
#             y = ts[:,-self.lookback + self.horizon:]
#             y = tensor(np.pad(
#                 y,
#                 pad_width=((0, 0), (self.lookback + self.horizon - y.shape[-1], 0)),
#                 mode='constant',
#                 constant_values=mean
#             ))
#             assert y.shape == (1,self.lookback+self.horizon), f"{y.shape}\t,{idx}, , 'tsshape':{ts.shape},'ts_id':{ts_id}"
#         else:
#             x = ts[:,lookback_id:lookback_id + self.lookback]
#             y = ts[:,lookback_id:lookback_id + self.lookback + self.horizon]
#         return x, y

#     def create_item(self, idx):
#         if idx>=self.n:
#             raise IndexError
#         x, y  = self.get_id(idx)
#         if self.norm:
#             x, y = self.normalize(x,y,idx)
#         if (y/(x.std()+1e-7)).std() > self.max_std:
#             if idx not in self.skipped:
# #                 print(f"idx: {idx};y.std to high: {(y/x.std()).std()} > {self.max_std}")
#                 self.skipped.append(idx)
#             raise SkipItemException()

#         return TSTensorSeq(x),TSTensorSeqy(y)

#     def normalize(self, x, y, idx, eps = 1e-7):
#         m, s = torch.mean(x,-1,keepdim=True), torch.std(x, -1,keepdim=True) + eps
#         self.ms[idx] = (m,s)
#         return (x-m)/s, (y-m)/s
