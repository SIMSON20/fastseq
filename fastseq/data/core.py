# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/03_data.core.ipynb (unless otherwise specified).

__all__ = ['NormalizeSeq', 'NormalizeSeqMulti', 'make_test', 'split_file', 'DfDataLoaders']

# Cell
from .load import *
from ..core import *
from fastcore.all import *
from fastcore.imports import *
from fastai2.basics import *
from fastai2.data.transforms import *
from fastai2.tabular.core import *
from .load import *
import orjson

# Cell
def _zeros_2_ones(o, eps=1e-8):
    nan_mask = o!=o
    o[o < eps ] = 1
    o[nan_mask ] = 1
    return o


# Cell
class NormalizeSeq(Transform):
    def __init__(self, verbose=False, make_ones=True, eps=1e-7, mean = None):
        store_attr(self,'verbose, make_ones, eps, mean')
        self.m, self.s = 0, 1

    def to_same_device(self, o):
        if o.is_cuda:
            self.m, self.s = to_device(self.m,'cuda'), to_device(self.s,'cuda')
        else:
            self.m, self.s = to_cpu(self.m), to_cpu(self.s)

    def encodes(self, o: TensorSeq):
        self.m = torch.mean(o, -1, keepdim=True)
        self.s = torch.std(o,  -1, keepdim=True) +self.eps
        if (self.s < self.eps*10).sum():
            self.s = _zeros_2_ones(self.s, self.eps*10)
        if self.verbose:
            print('encodes',[a.shape for a in o],
                  'm shape', {k:o.shape for k,o in self.m.items()},
                  's shape',{k:o.shape for k,o in self.s.items()})

        return self.norm(o)

    def norm(self, o):
        return (o - self.m)/self.s

    def decodes(self, o: TensorSeq):
        if self.verbose:
            print('decodes',o.shape,
                  'm shape',self.m.shape,
                  's shape',self.s.shape)
        return self.denorm(o)

    def denorm(self, o):
        self.to_same_device(o)
        return (o*self.s)+self.m

# Cell
class NormalizeSeqMulti(ItemTransform):
    """A shell Transformer to normalize `TensorSeqs` inside `TSMulti_` with `NormalizeSeqs`. """
    @delegates(NormalizeSeq.__init__)
    def __init__(self, n_its=5, **kwargs):
        """`n_its` does not include the ts to predict."""
        self.f = {i:NormalizeSeq(**kwargs) for i in range(n_its)}
        self.n = n_its

    def encodes(self, o:TSMulti_):
        r = L()
        for i,a in enumerate(o):
            if type(a) is not TensorSeq:
                r.append(a)
            elif i < (self.n-1):
                r.append(self.f[i](a))
            else:
                r.append(self.f[0].norm(o[i]))
        return TSMulti_(r)

    def decodes(self, o:TSMulti_):
        r = L(self.f[i].decode(a) for i,a in enumerate(o[:-1]))
        r.append(self.f[0].denorm(o[-1]))
        return TSMulti_(r)


# Cell
def make_test(ts:dict, horizon:int, lookback:int, keep_lookback:bool = False):
    """Splits the every ts in `items` based on `horizon + lookback`*, where the last part will go into `val` and the first in `train`.
    *if `keep_lookback`:
        it will only remove `horizon` from `train` otherwise will also remove lookback from `train`.
    """
    train,val = {},{}
    for k,v in ts.items():
        if k in ['ts_con','ts_cat']:
            if keep_lookback:
                train[k] = [o[:-(horizon)] for o in v]
            else:
                train[k] = [o[:-(horizon+lookback)] for o in v]
            val[k] = [o[-(horizon+lookback):] for o in v]
        elif k == '_length':
            train[k] = {k: v - (horizon if keep_lookback else horizon+lookback)}
            val[k] = {k:horizon+lookback}
        else:
            train[k] = v
            val[k] = v
    return train, val

# Cell
@delegates(make_test)
def split_file(file, clean=True, **kwargs):
    ts = get_ts_datapoint(file)
    t, v = make_test(ts, **kwargs)
    fs = []
    for f, dct in zip(['train','val'], [t,v]):
        new_f = Path(*str(file).split(os.sep)[:-1]) / f / str(file).split(os.sep)[-1]
        if not new_f.parent.exists(): new_f.parent.mkdir()
        open(new_f,'wb').write(orjson.dumps(t))
        fs.append(new_f)
    if clean:
        os.remove(file)
    return fs

# Cell
class DfDataLoaders(DataLoaders):
    @classmethod
    @delegates(DfDataLoader.__init__)
    def from_df(cls, dataset:pd.DataFrame, y_name:str, horizon:int, valid_pct=1.5, seed=None, lookback=None, step=1,
                   incl_test = True, path:Path='.', device=None, norm=True, **kwargs):
        """Create an list of time series.
        The `DataLoader` for the test set will be save as an attribute under `test`
        """
        lookback = ifnone(lookback, horizon * 4)
        device = ifnone(device, default_device())
        if incl_test:
            dataset, test = make_test_df(dataset, horizon, lookback, keep_lookback = True)
        train, valid = make_test_df(dataset, horizon + int(valid_pct*horizon), lookback , keep_lookback = True)
        if norm and 'after_batch' not in kwargs:
            kwargs.update({'after_batch':L(NormalizeSeqsMulti(n_its=4))})
        db = DataLoaders(*[DfDataLoader(ds, y_name, horizon=horizon, lookback=lookback, step=step,
                                        device=device, **kwargs)
                           for ds in [train,valid]], path=path, device=device)
        if incl_test:
            db.test = DfDataLoader(test, y_name, horizon=horizon, lookback=lookback, step=step, device=device, **kwargs)

            print(f"Train:{db.train.n}; Valid: {db.valid.n}; Test {db.test.n}")
        else:
            print(f"Train:{db.train.n}; Valid: {db.valid.n}")

        return db