# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/20_models.cnn.learner.ipynb (unless otherwise specified).

__all__ = ['adapt_conv']

# Cell
from fastcore.all import *
from fastai2.basics import *
from fastai2.vision import models
from fastai2.vision.learner import *
from torch.nn import Conv2d, Sequential, Module
from typing import Callable

# Cell
def adapt_conv(conv: Conv2d, n_channels:int, pretrained:bool=False,
               init=None, padding_mode:str='zeros'):
    '''Create a new layer that adapts `conv` to accept `n_channels` inputs.
       Copies existing weights if `pretrained` or initialises them with `init`.'''
    if conv.in_channels == n_channels: return conv # No need to adapt
    args = {n: getattr(conv, n) for n in ['kernel_size','stride','padding','dilation','groups']}
    bias = conv.bias is not None
    if 'padding_mode' in Conv2d.__constants__: # Padding mode added in PyTorch 1.1
        args['padding_mode'] = ifnone(padding_mode, conv.padding_mode)
    new_conv = Conv2d(n_channels, conv.out_channels, bias=bias, **args)
    if pretrained:
        exp_shape = (conv.out_channels, conv.in_channels, *conv.kernel_size)
        assert conv.weight.shape == exp_shape, f"Unexpected weights shape, expected {exp_shape}, got {conv.weight.shape}."
        new_conv.weight.data[...] = conv.weight.data[:,0:1,:,:]
        if bias: new_conv.bias.data = conv.bias.data
    elif init: init_default(new_conv, init)
    new_conv.to(conv.weight.device)
    return new_conv