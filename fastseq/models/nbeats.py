# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/10_models.nbeats.ipynb (unless otherwise specified).

__all__ = ['linspace', 'GenericBlock', 'seasonality_model', 'SeasonalityBlock', 'trend_model', 'TrendBlock',
           'NBeatsNet', 'NBeatsTrainer', 'nbeats_learner']

# Cell
from fastcore.utils import *
from fastcore.imports import *
from fastai2.basics import *
from fastai2.callback.hook import num_features_model
from fastai2.callback.all import *
from fastai2.torch_core import *
from torch.autograd import Variable
from ..all import *

# Cell

def linspace(lookback, horizon):
    lin_space = torch.linspace(
        -lookback, horizon, lookback + horizon
    )
    b_ls = lin_space[:lookback]
    f_ls = lin_space[lookback:]
    return b_ls, f_ls


# Cell
class GenericBlock(Module):
    def __init__(
        self, layers:L, thetas_dim:int, device, lookback=10, horizon=5, use_bn=True, bn_final=False, ps:L=None, share_thetas=False
    ):
        ps = ifnone(ps, L([0])*len(layers))
        sizes = [lookback] + layers
        actns = [nn.ReLU(inplace=True) for _ in range(len(sizes)-2)] + [None]
        _layers = [LinBnDrop(sizes[i], sizes[i+1], bn=use_bn, p=p, act=a)
                       for i,(p,a) in enumerate(zip(ps, actns))]
        self.layers = nn.Sequential(*_layers)

        if share_thetas:
            self.theta_f_fc = self.theta_b_fc = LinBnDrop(layers[-1], thetas_dim).to(device)
        else:
            self.theta_b_fc = LinBnDrop(layers[-1], thetas_dim).to(device)
            self.theta_f_fc = LinBnDrop(layers[-1], thetas_dim).to(device)

        self.backcast_fc = nn.Linear(thetas_dim, lookback).to(device)
        self.forecast_fc = nn.Linear(thetas_dim, horizon).to(device)

        b,f = linspace(lookback, horizon)
        self.backcast_linspace, self.forecast_linspace = Variable(b, requires_grad=False).to(device), Variable(f, requires_grad=False).to(device)
        self.to(device)

    def forward(self, x):
        # general
        x = self.layers(x)
        theta_b = self.theta_b_fc(x)
        theta_f = self.theta_f_fc(x)
        backcast = self.backcast_fc(theta_b)
        forecast = self.forecast_fc(theta_f)
        return {'b':backcast,'f': forecast,'theta': theta_b+theta_f}



# Cell

def seasonality_model(thetas, t):
    p = thetas.size()[-1]
    assert p < 10, "thetas_dim is too big."
    p1, p2 = (p // 2, p // 2) if p % 2 == 0 else (p // 2, p // 2 + 1)
    s1 = [torch.cos(2 * np.pi * i * t)[None,:] for i in range(p1)] # H/2-1
    s2 = [torch.sin(2 * np.pi * i * t)[None,:] for i in range(p2)]
    S = torch.cat([*s1, *s2])
    return thetas.mm(S)

class SeasonalityBlock(Module):
    def __init__(
        self, layers:L, thetas_dim:int, device, lookback=10, horizon=5, use_bn=True, bn_final=False, ps:L=None, share_thetas=False
    ):
        ps = ifnone(ps, L([0])*len(layers))
        sizes = [lookback] + layers
        actns = [nn.ReLU(inplace=True) for _ in range(len(sizes)-2)] + [None]
        _layers = [LinBnDrop(sizes[i], sizes[i+1], bn=use_bn, p=p, act=a)
                       for i,(p,a) in enumerate(zip(ps, actns))]
        self.layers = nn.Sequential(*_layers)

        if share_thetas:
            self.theta_f_fc = self.theta_b_fc = LinBnDrop(layers[-1], thetas_dim).to(device)
        else:
            self.theta_b_fc = LinBnDrop(layers[-1], thetas_dim).to(device)
            self.theta_f_fc = LinBnDrop(layers[-1], thetas_dim).to(device)

        b,f = linspace(lookback, horizon)
        self.backcast_linspace, self.forecast_linspace = Variable(b, requires_grad=False).to(device), Variable(f, requires_grad=False).to(device)
        self.to(device)

    def forward(self, x):
        # season
        x = self.layers(x)
        theta_b = self.theta_b_fc(x)
        theta_f = self.theta_f_fc(x)
        backcast = seasonality_model(theta_b, self.backcast_linspace)
        forecast = seasonality_model(theta_f, self.forecast_linspace)
        return {'b':backcast,'f': forecast,'theta': theta_b+theta_f}


# Cell
def trend_model(thetas, t):
    p = thetas.size()[-1]
    assert p <= 4, "thetas_dim is too big."
    a = [torch.pow(t, i)[None,:] for i in range(p)]
    T = torch.cat(a).float()
    return thetas.mm(T)

class TrendBlock(Module):
    def __init__(
        self, layers:L, thetas_dim:int, device, lookback=10, horizon=5, use_bn=True, bn_final=False, ps:L=None, share_thetas=False
    ):
        ps = ifnone(ps, L([0])*len(layers))
        sizes = [lookback] + layers
        actns = [nn.ReLU(inplace=True) for _ in range(len(sizes)-2)] + [None]
        _layers = [LinBnDrop(sizes[i], sizes[i+1], bn=use_bn, p=p, act=a)
                       for i,(p,a) in enumerate(zip(ps, actns))]
        self.layers = nn.Sequential(*_layers)

        if share_thetas:
            self.theta_f_fc = self.theta_b_fc = LinBnDrop(layers[-1], thetas_dim)
        else:
            self.theta_b_fc = LinBnDrop(layers[-1], thetas_dim)
            self.theta_f_fc = LinBnDrop(layers[-1], thetas_dim)

        b,f = linspace(lookback, horizon)
        self.backcast_linspace, self.forecast_linspace = Variable(b, requires_grad=False).to(device), Variable(f, requires_grad=False).to(device)
        self.to(device)

    def forward(self, x):
        # trend
        x = self.layers(x)
        theta_b = self.theta_b_fc(x)
        theta_f = self.theta_f_fc(x)
        backcast = trend_model(theta_b, self.backcast_linspace)
        forecast = trend_model(theta_f, self.forecast_linspace)
        return {'b':backcast,'f': forecast, 'theta': theta_b + theta_f}


# Cell
import numpy as np
import torch
from torch import nn
from torch.nn import functional as F


class NBeatsNet(Module):
    SEASONALITY_BLOCK = "seasonality"
    TREND_BLOCK = "trend"
    GENERIC_BLOCK = "generic"

    def __init__(
        self,
        device,
        stack_types=(TREND_BLOCK, SEASONALITY_BLOCK),
        nb_blocks_per_stack=3,
        horizon=5,
        lookback=10,
        thetas_dims=(4, 3),
        share_weights_in_stack=True,
        layers= [200,100],
    ):
        super(NBeatsNet, self).__init__()
        self.horizon = horizon
        self.lookback = lookback
        self.layers = layers
        self.nb_blocks_per_stack = nb_blocks_per_stack
        self.share_weights_in_stack = share_weights_in_stack
        self.stack_types = stack_types
        self.stacks = []
        self.thetas_dim = thetas_dims
        self.device = device
        self._str = "| N-Beats\n"

        self.bn = BatchNorm(lookback, ndim=2)
        stacks = OrderedDict()

        for stack_id in range(len(self.stack_types)):
            stacks[str(self.stack_types[stack_id]) + str(stack_id)] = self.create_stack(stack_id)
        self.stacks = nn.Sequential(stacks)
    def create_stack(self, stack_id):
        stack_type = self.stack_types[stack_id]
        self._str += f"| --  Stack {stack_type.title()} (#{stack_id}) (share_weights_in_stack={self.share_weights_in_stack})\n"

        blocks = []
        for block_id in range(self.nb_blocks_per_stack):
            block_init = NBeatsNet.select_block(stack_type)
            if self.share_weights_in_stack and block_id != 0:
                block = blocks[-1]  # pick up the last one when we share weights.
            else:
                block = block_init(
                    self.layers,
                    self.thetas_dim[stack_id],
                    self.device,
                    self.lookback,
                    self.horizon,
                )
            self._str += f"     | -- {block}\n"
            blocks.append(block)

        return nn.Sequential(*blocks)

    @staticmethod
    def select_block(block_type):
        if block_type == NBeatsNet.SEASONALITY_BLOCK:
            return SeasonalityBlock
        elif block_type == NBeatsNet.TREND_BLOCK:
            return TrendBlock
        else:
            return GenericBlock

    def forward(self, backcast):
        backcast = backcast.view([-1,backcast.shape[-1]])
        forecast = torch.zeros(
            size=(backcast.size()[0], self.horizon,)
        )  # maybe batch size here.

        dct = defaultdict(dict)
        for stack_id, names in enumerate(self.stacks.named_children()):
            name = names[0]
            for block_id in range(len(self.stacks[stack_id])):
                dct[name] = self.stacks[stack_id][block_id](backcast)
                backcast = backcast.to(self.device) - dct[name]['b']
                forecast = forecast.to(self.device) + dct[name]['f']
        return forecast[:,None,:], backcast[:,None,:], dct



# Cell
from fastai2.data.all import *
from fastai2.optimizer import *
from fastai2.learner import *

# Cell
class NBeatsTrainer(Callback):
    "`Callback` that adds weights regularization the thetas in N-Beats training."
    def __init__(self, alpha=0., beta=0.):
        self.alpha,self.beta = alpha,beta
        self.ratio = [0,0,0]

    def begin_train(self):
        self.ratio = [0,0,0]

    def begin_validate(self):
        self.out = []
        self.ratio = [0,0,0]

    def after_pred(self):
        if not self.training:
            self.out.append(self.pred[2])
        else:
            self.out = self.pred[2]
        self.learn.pred = self.pred[0]

    def after_loss(self):
        if not self.training: return
        self.ratio[0] += self.learn.loss.clone().detach()
        if self.alpha != 0.:
            self.learn.loss += self.alpha * self.out['trend0']['theta'][:,1:].float().pow(2).sum()
            self.learn.loss += self.alpha * self.out['seasonality1']['theta'].float().pow(2).sum()
            self.ratio[1] += self.learn.loss.clone().detach()
        if self.beta != 0.:
            self.learn.loss += self.beta * self.loss(self.out['b'].float(), self.xb, reduction='mean')
            self.ratio[2] += self.learn.loss.clone().detach()



# Cell
from fastai2.basics import *
from ..all import *


@delegates(NBeatsNet.__init__)
def nbeats_learner(dbunch:TSDataBunch, output_channels=None, metrics=None,cbs=None, alpha=0., beta=0., **kwargs):
    "Build a N-Beats style learner"
    model = NBeatsNet(
        device = dbunch.train_dl.device,
        horizon = dbunch.train_dl.horizon,
        lookback = dbunch.train_dl.lookback,
        **kwargs
       )
    model = model.to(dbunch.train_dl.device)

    learn = Learner(dbunch, model, loss_func=F.mse_loss, opt_func= Adam, metrics=L(metrics)+L(mae, smape), cbs=L(NBeatsTrainer(alpha,beta))+L(cbs))
    return learn