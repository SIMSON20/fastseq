{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.wavenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# hide\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wavenet model\n",
    "\n",
    "> The WaveNet` architecture for time series forecasting. <https://arxiv.org/pdf/1609.03499.pdf>\n",
    "\n",
    "Mostly copied from <https://github.com/MSRDL/Deep4Cast>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tako/dev/env3.7/lib/python3.7/site-packages/pandas/compat/__init__.py:85: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n",
      "/home/tako/dev/env3.7/lib/python3.7/site-packages/pandas/compat/__init__.py:85: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "from fastcore.utils import *\n",
    "from fastcore.imports import *\n",
    "from fastai2.basics import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class ConcreteDropout(torch.nn.Module):\n",
    "    \"\"\"Applies Dropout to the input, even at prediction time and learns dropout probability\n",
    "    from the data.\n",
    "    \n",
    "    In convolutional neural networks, we can use dropout to drop entire channels using\n",
    "    the 'channel_wise' argument.\n",
    "    \n",
    "    Arguments:\n",
    "        * dropout_regularizer (float): Should  be set to 2 / N, where N is the number of training examples.\n",
    "        * init_range (tuple): Initial range for dropout probabilities.\n",
    "        * channel_wise (boolean): apply dropout over all input or across convolutional channels.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 dropout_regularizer=1e-5,\n",
    "                 init_range=(0.1, 0.3),\n",
    "                 channel_wise=False):\n",
    "        super(ConcreteDropout, self).__init__()\n",
    "        self.dropout_regularizer = dropout_regularizer\n",
    "        self.init_range = init_range\n",
    "        self.channel_wise = channel_wise\n",
    "\n",
    "        # Initialize dropout probability\n",
    "        init_min = np.log(init_range[0]) - np.log(1. - init_range[0])\n",
    "        init_max = np.log(init_range[1]) - np.log(1. - init_range[1])\n",
    "        self.p_logit = torch.nn.Parameter(\n",
    "            torch.empty(1).uniform_(init_min, init_max))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Returns input but with randomly dropped out values.\"\"\"\n",
    "        # Get the dropout probability\n",
    "        p = torch.sigmoid(self.p_logit)\n",
    "\n",
    "        # Apply Concrete Dropout to input\n",
    "        out = self._concrete_dropout(x, p)\n",
    "\n",
    "        # Regularization term for dropout parameters\n",
    "        dropout_regularizer = p * torch.log(p)\n",
    "        dropout_regularizer += (1. - p) * torch.log(1. - p)\n",
    "\n",
    "        # The size of the dropout regularization depends on the kind of input\n",
    "        if self.channel_wise:\n",
    "            # Dropout only applied to channel dimension\n",
    "            input_dim = x.shape[1]\n",
    "        else:\n",
    "            # Dropout applied to all dimensions\n",
    "            input_dim = np.prod(x.shape[1:])\n",
    "        dropout_regularizer *= self.dropout_regularizer * input_dim\n",
    "\n",
    "        return out, dropout_regularizer.mean()\n",
    "\n",
    "    def _concrete_dropout(self, x, p):\n",
    "        # Empirical parameters for the concrete distribution\n",
    "        eps = 1e-7\n",
    "        temp = 0.1\n",
    "\n",
    "        # Apply Concrete dropout channel wise or across all input\n",
    "        if self.channel_wise:\n",
    "            unif_noise = torch.rand_like(x[:, :, [0]])\n",
    "        else:\n",
    "            unif_noise = torch.rand_like(x)\n",
    "\n",
    "        drop_prob = (torch.log(p + eps)\n",
    "                     - torch.log(1 - p + eps)\n",
    "                     + torch.log(unif_noise + eps)\n",
    "                     - torch.log(1 - unif_noise + eps))\n",
    "        drop_prob = torch.sigmoid(drop_prob / temp)\n",
    "        random_tensor = 1 - drop_prob\n",
    "\n",
    "        # Need to make sure we have the right shape for the Dropout mask\n",
    "        if self.channel_wise:\n",
    "            random_tensor = random_tensor.repeat([1, 1, x.shape[2]])\n",
    "\n",
    "        # Drop weights\n",
    "        retain_prob = 1 - p\n",
    "        x = torch.mul(x, random_tensor)\n",
    "        x /= retain_prob\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class WaveNet(torch.nn.Module):\n",
    "    \"\"\"Implements `WaveNet` architecture for time series forecasting. Inherits \n",
    "    from pytorch `Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`_.\n",
    "    Vector forecasts are made via a fully-connected linear layer.\n",
    "    References:\n",
    "        - `WaveNet: A Generative Model for Raw Audio <https://arxiv.org/pdf/1609.03499.pdf>`_\n",
    "    \n",
    "    Arguments:\n",
    "        * input_channels (int): Number of covariates in input time series.\n",
    "        * output_channels (int): Number of target time series.\n",
    "        * horizon (int): Number of time steps to forecast.\n",
    "        * hidden_channels (int): Number of channels in convolutional hidden layers.\n",
    "        * skip_channels (int): Number of channels in convolutional layers for skip connections.\n",
    "        * n_layers (int): Number of layers per Wavenet block (determines receptive field size).\n",
    "        * n_blocks (int): Number of Wavenet blocks.\n",
    "        * dilation (int): Dilation factor for temporal convolution.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_channels,\n",
    "                 output_channels,\n",
    "                 horizon,\n",
    "                 hidden_channels=64,\n",
    "                 skip_channels=64,\n",
    "                 n_layers=7,\n",
    "                 n_blocks=1,\n",
    "                 dilation=2):\n",
    "        \"\"\"Inititalize variables.\"\"\"\n",
    "        super(WaveNet, self).__init__()\n",
    "        self.output_channels = output_channels\n",
    "        self.horizon = horizon\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.skip_channels = skip_channels\n",
    "        self.n_layers = n_layers\n",
    "        self.n_blocks = n_blocks\n",
    "        self.dilation = dilation\n",
    "        self.dilations = [dilation**i for i in range(n_layers)] * n_blocks\n",
    "\n",
    "        # Set up first layer for input\n",
    "        self.do_conv_input = ConcreteDropout(channel_wise=True)\n",
    "        self.conv_input = torch.nn.Conv1d(\n",
    "            in_channels=input_channels,\n",
    "            out_channels=hidden_channels,\n",
    "            kernel_size=1\n",
    "        )\n",
    "\n",
    "        # Set up main WaveNet layers\n",
    "        self.do, self.conv, self.skip, self.resi = [], [], [], []\n",
    "        for d in self.dilations:\n",
    "            self.do.append(ConcreteDropout(channel_wise=True))\n",
    "            self.conv.append(torch.nn.Conv1d(in_channels=hidden_channels,\n",
    "                                             out_channels=hidden_channels,\n",
    "                                             kernel_size=2,\n",
    "                                             dilation=d))\n",
    "            self.skip.append(torch.nn.Conv1d(in_channels=hidden_channels,\n",
    "                                             out_channels=skip_channels,\n",
    "                                             kernel_size=1))\n",
    "            self.resi.append(torch.nn.Conv1d(in_channels=hidden_channels,\n",
    "                                             out_channels=hidden_channels,\n",
    "                                             kernel_size=1))\n",
    "        self.do = torch.nn.ModuleList(self.do)\n",
    "        self.conv = torch.nn.ModuleList(self.conv)\n",
    "        self.skip = torch.nn.ModuleList(self.skip)\n",
    "        self.resi = torch.nn.ModuleList(self.resi)\n",
    "\n",
    "        # Set up nonlinear output layers\n",
    "        self.do_conv_post = ConcreteDropout(channel_wise=True)\n",
    "        self.conv_post = torch.nn.Conv1d(\n",
    "            in_channels=skip_channels,\n",
    "            out_channels=skip_channels,\n",
    "            kernel_size=1\n",
    "        )\n",
    "        self.do_linear_mean = ConcreteDropout()\n",
    "        self.do_linear_std = ConcreteDropout()\n",
    "        self.do_linear_df = ConcreteDropout()\n",
    "        self.linear_mean = torch.nn.Linear(\n",
    "            skip_channels, horizon*output_channels)\n",
    "        self.linear_std = torch.nn.Linear(\n",
    "            skip_channels, horizon*output_channels)\n",
    "        self.linear_df = torch.nn.Linear(\n",
    "            skip_channels, horizon*output_channels)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward function.\"\"\"\n",
    "        output, reg_e = self.encode(inputs)\n",
    "        output_mean, output_std, output_df, reg_d = self.decode(output)\n",
    "\n",
    "        # Regularization\n",
    "        regularizer = reg_e + reg_d\n",
    "\n",
    "        return output_df # , 'loc': output_mean, 'scale': output_std, 'regularizer': regularizer}\n",
    "\n",
    "    def encode(self, inputs: torch.Tensor):\n",
    "        \"\"\"Returns embedding vectors.\n",
    "        \n",
    "        Arguments:\n",
    "            * inputs: time series input to make forecasts for\n",
    "        \"\"\"\n",
    "        # Input layer\n",
    "        output, res_conv_input = self.do_conv_input(inputs)\n",
    "        output = self.conv_input(output)\n",
    "\n",
    "        # Loop over WaveNet layers and blocks\n",
    "        regs, skip_connections = [], []\n",
    "        for do, conv, skip, resi in zip(self.do, self.conv, self.skip, self.resi):\n",
    "            layer_in = output\n",
    "            output, reg = do(layer_in)\n",
    "            output = conv(output)\n",
    "            output = torch.nn.functional.relu(output)\n",
    "            skip = skip(output)\n",
    "            output = resi(output)\n",
    "            output = output + layer_in[:, :, -output.size(2):]\n",
    "            regs.append(reg)\n",
    "            skip_connections.append(skip)\n",
    "\n",
    "        # Sum up regularizer terms and skip connections\n",
    "        regs = sum(r for r in regs)\n",
    "        output = sum([s[:, :, -output.size(2):] for s in skip_connections])\n",
    "\n",
    "        # Nonlinear output layers\n",
    "        output, res_conv_post = self.do_conv_post(output)\n",
    "        output = torch.nn.functional.relu(output)\n",
    "        output = self.conv_post(output)\n",
    "        output = torch.nn.functional.relu(output)\n",
    "        output = output[:, :, [-1]]\n",
    "        output = output.transpose(1, 2)\n",
    "\n",
    "        # Regularization terms\n",
    "        regularizer = res_conv_input \\\n",
    "            + regs \\\n",
    "            + res_conv_post\n",
    "\n",
    "        return output, regularizer\n",
    "\n",
    "    def decode(self, inputs: torch.Tensor):\n",
    "        \"\"\"Returns forecasts based on embedding vectors.\n",
    "        \n",
    "        Arguments:\n",
    "            * inputs: embedding vectors to generate forecasts for\n",
    "        \"\"\"\n",
    "        # Apply dense layer to match output length\n",
    "        output_mean, res_linear_mean = self.do_linear_mean(inputs)\n",
    "        output_std, res_linear_std = self.do_linear_std(inputs)\n",
    "        output_df, res_linear_df = self.do_linear_df(inputs)\n",
    "        output_mean = self.linear_mean(output_mean)\n",
    "        output_std = self.linear_std(output_std).exp()\n",
    "        output_df = self.linear_df(output_df).exp()\n",
    "\n",
    "        # Reshape the layer output to match targets\n",
    "        # Shape is (batch_size, output_channels, horizon)\n",
    "        batch_size = inputs.shape[0]\n",
    "        output_mean = output_mean.reshape(\n",
    "            (batch_size, self.output_channels, self.horizon)\n",
    "        )\n",
    "        output_std = output_std.reshape(\n",
    "            (batch_size, self.output_channels, self.horizon)\n",
    "        )\n",
    "        output_df = output_df.reshape(\n",
    "            (batch_size, self.output_channels, self.horizon)\n",
    "        )\n",
    "\n",
    "        # Regularization terms\n",
    "        regularizer = res_linear_mean + res_linear_std + res_linear_df\n",
    "\n",
    "        return output_mean, output_std, output_df, regularizer\n",
    "\n",
    "    @property\n",
    "    def n_parameters(self):\n",
    "        \"\"\"Returns the number of model parameters.\"\"\"\n",
    "        par = list(self.parameters())\n",
    "        s = sum([np.prod(list(d.size())) for d in par])\n",
    "        return s\n",
    "\n",
    "    @property\n",
    "    def receptive_field_size(self):\n",
    "        \"\"\"Returns the length of the receptive field.\"\"\"\n",
    "        return self.dilation * max(self.dilations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastseq.data.load import *\n",
    "from fastseq.data.transforms import *\n",
    "from fastai2.data.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 12\n",
    "lookback = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 20), (2, 10), (2, 140), (2, 140), (2, 200)]\n"
     ]
    }
   ],
   "source": [
    "t = np.arange(1000)\n",
    "lenghts = [20,10,140,140,200]\n",
    "data_train = [np.array([i+.5*np.sin(t[:l]),\n",
    "              t[:l]+(0.1*np.random.randn()),\n",
    "             ])\n",
    "              for i,l in enumerate(lenghts)]\n",
    "\n",
    "print([d.shape for d in data_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 1/5 time series due to length.\n"
     ]
    }
   ],
   "source": [
    "ts_ds = TimeSeriesDataset(\n",
    "    data_train,\n",
    "    lookback,\n",
    "    horizon,\n",
    "    step=1,\n",
    "    static_covs = [1,2,2,2,2],\n",
    "    transform = ToTensor()\n",
    ")\n",
    "ts_dl = DataLoader(\n",
    "    ts_ds,\n",
    "    batch_size=512,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model parameters: 125076.\n",
      "Receptive field size: 128.\n"
     ]
    }
   ],
   "source": [
    "model = WaveNet(input_channels=2,\n",
    "                output_channels=2,\n",
    "                horizon=horizon,\n",
    "               )\n",
    "\n",
    "print('Number of model parameters: {}.'.format(model.n_parameters))\n",
    "print('Receptive field size: {}.'.format(model.receptive_field_size))\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.0008097436666349985)\n",
    "\n",
    "# # .. and the loss\n",
    "# loss = torch.distributions.StudentT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataBunch(ts_dl, ts_dl).cuda()\n",
    "learn = Learner(data, model, loss_func=F.mse_loss, opt_func=Adam, metrics=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/tako/dev/env3.7/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/tako/dev/env3.7/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 34, in fetch\n    data = next(self.dataset_iter)\n  File \"/home/tako/dev/fastai2/fastai2/data/load.py\", line 106, in create_batches\n    yield from map(self.do_batch, self.chunkify(res))\n  File \"/home/tako/dev/fastcore/fastcore/utils.py\", line 250, in chunked\n    res = list(itertools.islice(it, cs))\n  File \"/home/tako/dev/fastai2/fastai2/data/load.py\", line 119, in do_item\n    try: return self.after_item(self.create_item(s))\n  File \"/home/tako/dev/fastai2/fastai2/data/load.py\", line 125, in create_item\n    def create_item(self, s):  return next(self.it) if s is None else self.dataset[s]\n  File \"../fastseq/data/load.py\", line 113, in __getitem__\n    print('applying transform')\n  File \"/home/tako/dev/fastcore/fastcore/transform.py\", line 61, in __call__\n    def __call__(self, x, **kwargs): return self._call('encodes', x, **kwargs)\n  File \"/home/tako/dev/fastcore/fastcore/transform.py\", line 69, in _call\n    if self.use_as_item or not is_listy(x): return self._do_call(f, x, **kwargs)\n  File \"/home/tako/dev/fastcore/fastcore/transform.py\", line 74, in _do_call\n    return x if f is None else retain_type(f(x, **kwargs), x, f.returns_none(x))\n  File \"/home/tako/dev/fastcore/fastcore/dispatch.py\", line 98, in __call__\n    return f(*args, **kwargs)\n  File \"../fastseq/data/transforms.py\", line 12, in encodes\n    return tensor(o)\n  File \"/home/tako/dev/fastai2/fastai2/torch_core.py\", line 111, in tensor\n    else as_tensor(x, **kwargs) if hasattr(x, '__array__') or is_iter(x)\nRuntimeError: Could not infer dtype of dict\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-bd8b18fd11a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfastai2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/dev/fastai2/fastai2/callback/schedule.py\u001b[0m in \u001b[0;36mlr_find\u001b[0;34m(self, start_lr, end_lr, num_it, stop_div, show_plot)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0mn_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_it\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdbunch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mcb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLRFinder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_it\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_it\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_div\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop_div\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_logging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshow_plot\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_lr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/fastai2/fastai2/learner.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, n_epoch, lr, wd, cbs, reset_opt)\u001b[0m\n\u001b[1;32m    286\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m;\u001b[0m          \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'begin_epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_epoch_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_epoch_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mCancelEpochException\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_cancel_epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/fastai2/fastai2/learner.py\u001b[0m in \u001b[0;36m_do_epoch_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdbunch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m;\u001b[0m                  \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'begin_train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCancelTrainException\u001b[0m\u001b[0;34m:\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_cancel_train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/fastai2/fastai2/learner.py\u001b[0m in \u001b[0;36mall_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mall_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mone_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/fastai2/fastai2/data/load.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandomize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_loaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfake_l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfake_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/env3.7/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/env3.7/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    844\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/env3.7/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/tako/dev/env3.7/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/tako/dev/env3.7/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 34, in fetch\n    data = next(self.dataset_iter)\n  File \"/home/tako/dev/fastai2/fastai2/data/load.py\", line 106, in create_batches\n    yield from map(self.do_batch, self.chunkify(res))\n  File \"/home/tako/dev/fastcore/fastcore/utils.py\", line 250, in chunked\n    res = list(itertools.islice(it, cs))\n  File \"/home/tako/dev/fastai2/fastai2/data/load.py\", line 119, in do_item\n    try: return self.after_item(self.create_item(s))\n  File \"/home/tako/dev/fastai2/fastai2/data/load.py\", line 125, in create_item\n    def create_item(self, s):  return next(self.it) if s is None else self.dataset[s]\n  File \"../fastseq/data/load.py\", line 113, in __getitem__\n    print('applying transform')\n  File \"/home/tako/dev/fastcore/fastcore/transform.py\", line 61, in __call__\n    def __call__(self, x, **kwargs): return self._call('encodes', x, **kwargs)\n  File \"/home/tako/dev/fastcore/fastcore/transform.py\", line 69, in _call\n    if self.use_as_item or not is_listy(x): return self._do_call(f, x, **kwargs)\n  File \"/home/tako/dev/fastcore/fastcore/transform.py\", line 74, in _do_call\n    return x if f is None else retain_type(f(x, **kwargs), x, f.returns_none(x))\n  File \"/home/tako/dev/fastcore/fastcore/dispatch.py\", line 98, in __call__\n    return f(*args, **kwargs)\n  File \"../fastseq/data/transforms.py\", line 12, in encodes\n    return tensor(o)\n  File \"/home/tako/dev/fastai2/fastai2/torch_core.py\", line 111, in tensor\n    else as_tensor(x, **kwargs) if hasattr(x, '__array__') or is_iter(x)\nRuntimeError: Could not infer dtype of dict\n"
     ]
    }
   ],
   "source": [
    "from fastai2.callback.all import *\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_data.external.ipynb.\n",
      "Converted 02_data.load.ipynb.\n",
      "This cell doesn't have an export destination and was ignored:\n",
      " \n",
      "Converted 02_deep4cast_m4_example.ipynb.\n",
      "Converted 03_data.load.ipynb.\n",
      "Converted 04_models.wavenet.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python37",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
