{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data.transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tako/dev/env37/lib/python3.7/site-packages/pandas/compat/__init__.py:85: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n",
      "/home/tako/dev/env37/lib/python3.7/site-packages/pandas/compat/__init__.py:85: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# hide \n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai2.torch_basics import *\n",
    "from fastai2.data.all import *\n",
    "# from pyts.image import GramianAngularField, MarkovTransitionField, RecurrencePlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforms\n",
    "\n",
    "> Basic function to process time-series data before assembling it in a `DataLoaders`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "@ToTensor\n",
    "def encodes(self,o):\n",
    "    return tensor(o).float()\n",
    "@ToTensor\n",
    "def decodes(self,o)->np.ndarray:\n",
    "    return np.array(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# TODO normalize proc\n",
    "def norm_items(self, items, norm):\n",
    "    items = items.map(tensor)\n",
    "    r=L()\n",
    "    for i,ts in enumerate(items):\n",
    "        ts = ts.float()\n",
    "        if norm:\n",
    "            ts = (ts - torch.mean(ts.float(), -1, keepdim = True))/(torch.std(ts.float(), -1, keepdim = True)+1e-8)\n",
    "        r.append(ts)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmf = ToTensor()\n",
    "test_eq(tmf(np.array([1])), tensor([1]).float())\n",
    "test_eq(tmf((np.array(1),np.array(1))),(tensor(1),tensor(1)))\n",
    "test_eq(tmf.decode((tensor(1),tensor(1))),(np.array(1),np.array(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_data.external.ipynb.\n",
      "Converted 02_data.load.ipynb.\n",
      "Converted 03_data.core.ipynb.\n",
      "Converted 03_data.transforms.ipynb.\n",
      "Converted 04_metrics.ipynb.\n",
      "Converted 05_models.nbeats.ipynb.\n",
      "Converted 07_interpret.ipynb.\n",
      "Converted 100_readme_alt.ipynb.\n",
      "Converted 21_data.loadpandas.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env37",
   "language": "python",
   "name": "env37"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
